{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T22:11:42.368220Z",
     "iopub.status.busy": "2025-03-27T22:11:42.367812Z",
     "iopub.status.idle": "2025-03-27T22:11:42.372615Z",
     "shell.execute_reply": "2025-03-27T22:11:42.371395Z",
     "shell.execute_reply.started": "2025-03-27T22:11:42.368191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = \"voir mail\"\n",
    "MISTRAL_API_KEY = \"voir mail\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T22:13:11.663253Z",
     "iopub.status.busy": "2025-03-27T22:13:11.662789Z",
     "iopub.status.idle": "2025-03-27T22:14:47.897387Z",
     "shell.execute_reply": "2025-03-27T22:14:47.896063Z",
     "shell.execute_reply.started": "2025-03-27T22:13:11.663215Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-postprocessor (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-postprocessor\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-query-engine (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-query-engine\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-retrievers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-retrievers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-core-retrievers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-core-retrievers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-core-post-processor (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-core-post-processor\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-core-schema (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-core-schema\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.2.0\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.2.0\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.2.0\u001b[24m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai > /dev/null\n",
    "!pip install llama-index > /dev/null\n",
    "!pip install llama-index-core > /dev/null\n",
    "!pip install llama-index-vector-stores-qdrant > /dev/null\n",
    "!pip install llama-index-postprocessor > /dev/null\n",
    "!pip install llama-index-query-engine > /dev/null\n",
    "!pip install llama-index-embeddings-huggingface > /dev/null\n",
    "!pip install llama-index-llms-ollama > /dev/null\n",
    "!pip install llama-index-llms-mistralai > /dev/null\n",
    "!pip install qdrant-client > /dev/null\n",
    "!pip install llama-index-retrievers > /dev/null\n",
    "!pip install llama-index-core-retrievers > /dev/null\n",
    "!pip install llama-index-retrievers-bm25 > /dev/null\n",
    "!pip install llama-index-core-post-processor > /dev/null\n",
    "!pip install llama-index-core-schema > /dev/null\n",
    "!pip install streamlit > /dev/null\n",
    "!npm install localtunnel > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T22:14:47.899952Z",
     "iopub.status.busy": "2025-03-27T22:14:47.899367Z",
     "iopub.status.idle": "2025-03-27T22:15:26.440190Z",
     "shell.execute_reply": "2025-03-27T22:15:26.439293Z",
     "shell.execute_reply.started": "2025-03-27T22:14:47.899905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gc\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "import Stemmer\n",
    "from llama_index.core import VectorStoreIndex, QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T22:15:26.442439Z",
     "iopub.status.busy": "2025-03-27T22:15:26.441727Z",
     "iopub.status.idle": "2025-03-27T22:15:37.184818Z",
     "shell.execute_reply": "2025-03-27T22:15:37.183720Z",
     "shell.execute_reply.started": "2025-03-27T22:15:26.442408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98914c73b9b5491c81621cc97c986d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f19f138cafb418f92ce60f8d2296494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309cf63d0ac0421890f48472f796b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2447a277034242458a75195d3007c93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40fc2ccd8b74981bbafe21529b1414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47b20fd4de64a7686706d261b4d671f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d06a418264a74999dd592745d31e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07089a4182c7432080a8dd385ca00215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f99ed960c24490a639f7f39992d658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed0c7599c464ca381557602784381e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57eef4dc5584397bc631ac1595f5e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "    \"https://3cf7f5eb-3647-41f9-94aa-6dbc76a973d4.us-west-1-0.aws.cloud.qdrant.io:6333\",\n",
    "    port = 6333,\n",
    "    api_key = QDRANT_API_KEY\n",
    ")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"baai/bge-small-en-v1.5\")\n",
    "Settings.llm = MistralAI(api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T22:37:44.259883Z",
     "iopub.status.busy": "2025-03-27T22:37:44.259514Z",
     "iopub.status.idle": "2025-03-27T22:37:44.265228Z",
     "shell.execute_reply": "2025-03-27T22:37:44.264097Z",
     "shell.execute_reply.started": "2025-03-27T22:37:44.259853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pkl(path, obj2save):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj2save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T23:07:16.913617Z",
     "iopub.status.busy": "2025-03-27T23:07:16.913242Z",
     "iopub.status.idle": "2025-03-27T23:07:16.921673Z",
     "shell.execute_reply": "2025-03-27T23:07:16.920525Z",
     "shell.execute_reply.started": "2025-03-27T23:07:16.913590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Qdrant_DB:\n",
    "    def __init__(self, client: QdrantClient, collection_name=\"data-battle-v3\") -> None:\n",
    "        self.client = client\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_store = self.init_qdrant_vector_store()\n",
    "        self.vector_index = self.init_vector_index()\n",
    "        self.retriever = self.init_retriever()\n",
    "        \n",
    "    # Init Qdrant vector store\n",
    "    def init_qdrant_vector_store(self):\n",
    "        return QdrantVectorStore(client=self.client, collection_name=self.collection_name)\n",
    "\n",
    "    # create an index\n",
    "    def init_vector_index(self):\n",
    "        return VectorStoreIndex.from_vector_store(self.vector_store)\n",
    "        \n",
    "    def init_retriever(self, similarity_top_k=2):\n",
    "        return self.vector_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    \n",
    "    def add_nodes(self, nodes):\n",
    "        self.vector_store.upsert(nodes)\n",
    "        self.vector_index = self.init_vector_index()\n",
    "        self.retriever = self.init_retriever()\n",
    "        \n",
    "    def get_nodes(self):\n",
    "        return self.vector_store.get_nodes()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle):\n",
    "        return self.retriever.retrieve(query_bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T01:18:55.379663Z",
     "iopub.status.busy": "2025-03-28T01:18:55.379349Z",
     "iopub.status.idle": "2025-03-28T01:18:55.393091Z",
     "shell.execute_reply": "2025-03-28T01:18:55.391787Z",
     "shell.execute_reply.started": "2025-03-28T01:18:55.379638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingBM25RerankerRetriever(BaseRetriever) :\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        bm25_retriever: BM25Retriever,\n",
    "        reranker: SentenceTransformerRerank,\n",
    "    ) -> None:\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.reranker = reranker\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_nodes.extend(bm25_nodes)\n",
    "\n",
    "        retrieved_nodes = self.reranker.postprocess_nodes(\n",
    "            vector_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "        return retrieved_nodes\n",
    "\n",
    "# create the Qdrant retriever \n",
    "def create_embedding_retriever(qdrant_db: Qdrant_DB, similarity_top_k=50) :\n",
    "    return qdrant_db.init_retriever(similarity_top_k)\n",
    "\n",
    "# create the bm25 retriever for a given list of nodes\n",
    "def create_bm25_retriever(nodes, similarity_top_k=15) :\n",
    "    return BM25Retriever.from_defaults(\n",
    "        nodes = nodes,\n",
    "        similarity_top_k = similarity_top_k,\n",
    "        stemmer = Stemmer.Stemmer(\"english\"),\n",
    "        language = \"english\",\n",
    "    )\n",
    "\n",
    "# create the full retriever \n",
    "def create_full_retriever(qdrant_db: Qdrant_DB, emb_k=50, bm25_k=15, reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", reranker_n=6) :\n",
    "    # create the embedding retriever \n",
    "    embedding_retriever = create_embedding_retriever(qdrant_db, similarity_top_k=emb_k)\n",
    "\n",
    "    # get nodes for the BM25 retriever\n",
    "    nodes = qdrant_db.get_nodes()\n",
    "\n",
    "    # create the BM25 retriever\n",
    "    bm25_retriever = create_bm25_retriever(\n",
    "        nodes, similarity_top_k=bm25_k\n",
    "    )\n",
    "\n",
    "    # create the reranker\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=reranker_model, top_n=reranker_n\n",
    "    )\n",
    "\n",
    "    # fusion retrievers and add the reranker\n",
    "    embedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n",
    "        embedding_retriever, bm25_retriever, reranker=reranker\n",
    "    )\n",
    "    \n",
    "    return embedding_bm25_retriever_rerank\n",
    "\n",
    "# create the retriever based on the retriever_name argument\n",
    "def create_chosen_retriever(qdrant_db: Qdrant_DB, retriever_name=\"Embedding/BM25 retrievers + rerank\", emb_k=50, bm25_k=15, reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", reranker_n=6) :\n",
    "    # create the embedding retriever \n",
    "    if retriever_name == \"Embedding retriever\":\n",
    "        return create_embedding_retriever(qdrant_db, similarity_top_k=emb_k)\n",
    "\n",
    "    # create the BM25 retriever\n",
    "    elif retriever_name == \"BM25 retriever\":\n",
    "        nodes = qdrant_db.get_nodes()\n",
    "        return create_bm25_retriever(nodes, similarity_top_k=bm25_k)\n",
    "\n",
    "    # create the full retriever\n",
    "    else:\n",
    "        return create_full_retriever(qdrant_db, emb_k=emb_k, bm25_k=bm25_k, reranker_model=reranker_model, reranker_n=reranker_n)\n",
    "\n",
    "# retrieve context and sources \n",
    "def retrieve_context_and_sources(retriever, query, show_scores=False):\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    # concatenate the chunks to have the context \n",
    "    context = \"\\n*********\\n\".join([node.text for node in retrieved_nodes])\n",
    "\n",
    "    # get sources from the chunks \n",
    "    sources = []\n",
    "    for node in retrieved_nodes:\n",
    "        file_name = node.metadata.get(\"file_name\", \"Inconnu\")  \n",
    "        page_label = node.metadata.get(\"page_label\")  \n",
    "\n",
    "        if page_label:\n",
    "            sources.append(f\"{file_name} (Page {page_label})\")\n",
    "        else:\n",
    "            sources.append(file_name)\n",
    "        \n",
    "        if show_scores:\n",
    "            sources.append(f\" - Score: {node.score:.2f}\")\n",
    "\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of use \n",
    "### *(only retrieve documents, doesn't generate answers)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T01:18:55.394906Z",
     "iopub.status.busy": "2025-03-28T01:18:55.394472Z",
     "iopub.status.idle": "2025-03-28T01:19:19.276898Z",
     "shell.execute_reply": "2025-03-28T01:19:19.275631Z",
     "shell.execute_reply.started": "2025-03-28T01:18:55.394864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed6a3035b4c4950bcb7a0951fb2851c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4339a6be718e403197d6160feb861b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\n",
    "    \"https://3cf7f5eb-3647-41f9-94aa-6dbc76a973d4.us-west-1-0.aws.cloud.qdrant.io:6333\",\n",
    "    port = 6333,\n",
    "    api_key = QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "qdrant_db = Qdrant_DB(client)\n",
    "# full retriever by default\n",
    "retriever = create_chosen_retriever(qdrant_db)\n",
    "\n",
    "query = \"What are European Patent Application?\"\n",
    "\n",
    "context, sources = retrieve_context_and_sources(retriever, query, show_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T00:19:18.403502Z",
     "iopub.status.busy": "2025-03-28T00:19:18.403116Z",
     "iopub.status.idle": "2025-03-28T00:19:18.410571Z",
     "shell.execute_reply": "2025-03-28T00:19:18.409538Z",
     "shell.execute_reply.started": "2025-03-28T00:19:18.403473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing api_key.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api_key.py\n",
    "\n",
    "QDRANT_API_KEY = \"voir mail\"\n",
    "MISTRAL_API_KEY = \"voir mail\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# database.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T00:25:22.882998Z",
     "iopub.status.busy": "2025-03-28T00:25:22.882579Z",
     "iopub.status.idle": "2025-03-28T00:25:22.890085Z",
     "shell.execute_reply": "2025-03-28T00:25:22.888898Z",
     "shell.execute_reply.started": "2025-03-28T00:25:22.882965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting database.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile database.py\n",
    "\n",
    "import pickle\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, QueryBundle\n",
    "\n",
    "class Qdrant_DB:\n",
    "    def __init__(self, client: QdrantClient, collection_name=\"data-battle-v3\") -> None:\n",
    "        self.client = client\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_store = self.init_qdrant_vector_store()\n",
    "        self.vector_index = self.init_vector_index()\n",
    "        self.retriever = self.init_retriever()\n",
    "        \n",
    "    # Init Qdrant vector store\n",
    "    def init_qdrant_vector_store(self):\n",
    "        return QdrantVectorStore(client=self.client, collection_name=self.collection_name)\n",
    "\n",
    "    # create an index\n",
    "    def init_vector_index(self):\n",
    "        return VectorStoreIndex.from_vector_store(self.vector_store)\n",
    "        \n",
    "    def init_retriever(self, similarity_top_k=2):\n",
    "        return self.vector_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    \n",
    "    def add_nodes(self, nodes):\n",
    "        self.vector_store.upsert(nodes)\n",
    "        self.vector_index = self.init_vector_index()\n",
    "        self.retriever = self.init_retriever()\n",
    "        \n",
    "    def get_nodes(self):\n",
    "        return self.vector_store.get_nodes()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle):\n",
    "        return self.retriever.retrieve(query_bundle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retriever.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T01:19:19.288505Z",
     "iopub.status.busy": "2025-03-28T01:19:19.288175Z",
     "iopub.status.idle": "2025-03-28T01:19:19.295786Z",
     "shell.execute_reply": "2025-03-28T01:19:19.294631Z",
     "shell.execute_reply.started": "2025-03-28T01:19:19.288476Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting retriever.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile retriever.py\n",
    "\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, QueryBundle, StorageContext\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from typing import List\n",
    "import Stemmer\n",
    "import gc\n",
    "\n",
    "import database\n",
    "\n",
    "class EmbeddingBM25RerankerRetriever(BaseRetriever) :\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        bm25_retriever: BM25Retriever,\n",
    "        reranker: SentenceTransformerRerank,\n",
    "    ) -> None:\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.reranker = reranker\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_nodes.extend(bm25_nodes)\n",
    "\n",
    "        retrieved_nodes = self.reranker.postprocess_nodes(\n",
    "            vector_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "        return retrieved_nodes\n",
    "\n",
    "# create the Qdrant retriever \n",
    "def create_embedding_retriever(qdrant_db: database.Qdrant_DB, similarity_top_k=50) :\n",
    "    return qdrant_db.init_retriever(similarity_top_k)\n",
    "\n",
    "# create the bm25 retriever for a given list of nodes\n",
    "def create_bm25_retriever(nodes, similarity_top_k=15) :\n",
    "    return BM25Retriever.from_defaults(\n",
    "        nodes = nodes,\n",
    "        similarity_top_k = similarity_top_k,\n",
    "        stemmer = Stemmer.Stemmer(\"english\"),\n",
    "        language = \"english\",\n",
    "    )\n",
    "\n",
    "# create the full retriever \n",
    "def create_full_retriever(qdrant_db: database.Qdrant_DB, emb_k=50, bm25_k=15, reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", reranker_n=6) :\n",
    "    # create the embedding retriever \n",
    "    embedding_retriever = create_embedding_retriever(qdrant_db, similarity_top_k=emb_k)\n",
    "\n",
    "    # get nodes for the BM25 retriever\n",
    "    nodes = qdrant_db.get_nodes()\n",
    "\n",
    "    # create the BM25 retriever\n",
    "    bm25_retriever = create_bm25_retriever(\n",
    "        nodes, similarity_top_k=bm25_k\n",
    "    )\n",
    "\n",
    "    # create the reranker\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=reranker_model, top_n=reranker_n\n",
    "    )\n",
    "\n",
    "    # fusion retrievers and add the reranker\n",
    "    embedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n",
    "        embedding_retriever, bm25_retriever, reranker=reranker\n",
    "    )\n",
    "    \n",
    "    return embedding_bm25_retriever_rerank\n",
    "\n",
    "# create the retriever based on the retriever_name argument\n",
    "def create_chosen_retriever(qdrant_db: database.Qdrant_DB, retriever_name=\"Embedding/BM25 retrievers + rerank\", emb_k=50, bm25_k=15, reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", reranker_n=6) :\n",
    "    # create the embedding retriever \n",
    "    if retriever_name == \"Embedding retriever\":\n",
    "        return create_embedding_retriever(qdrant_db, similarity_top_k=emb_k)\n",
    "\n",
    "    # create the BM25 retriever\n",
    "    elif retriever_name == \"BM25 retriever\":\n",
    "        nodes = qdrant_db.get_nodes()\n",
    "        return create_bm25_retriever(nodes, similarity_top_k=bm25_k)\n",
    "\n",
    "    # create the full retriever\n",
    "    else:\n",
    "        return create_full_retriever(qdrant_db, emb_k=emb_k, bm25_k=bm25_k, reranker_model=reranker_model, reranker_n=reranker_n)\n",
    "\n",
    "# retrieve context and sources \n",
    "def retrieve_context_and_sources(retriever, query, show_scores=False):\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "\n",
    "    # concatenate the chunks to have the context \n",
    "    context = \"\\n*********\\n\".join([node.text for node in retrieved_nodes])\n",
    "\n",
    "    # get sources from the chunks \n",
    "    sources = []\n",
    "    for node in retrieved_nodes:\n",
    "        file_name = node.metadata.get(\"file_name\", \"Inconnu\")  \n",
    "        page_label = node.metadata.get(\"page_label\")\n",
    "        \n",
    "        if page_label:\n",
    "            sources.append(f\"{file_name} (Page {page_label})\")\n",
    "        else:\n",
    "            sources.append(file_name)\n",
    "        if show_scores:\n",
    "            sources.append(f\" - Score: {node.score:.2f}\")\n",
    "\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memory.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T00:25:29.927842Z",
     "iopub.status.busy": "2025-03-28T00:25:29.927499Z",
     "iopub.status.idle": "2025-03-28T00:25:29.934360Z",
     "shell.execute_reply": "2025-03-28T00:25:29.933183Z",
     "shell.execute_reply.started": "2025-03-28T00:25:29.927815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting memory.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile memory.py\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.memory import ChatSummaryMemoryBuffer\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "import api_key\n",
    "\n",
    "# initialisation\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "Settings.llm = MistralAI(api_key=api_key.MISTRAL_API_KEY)\n",
    "\n",
    "# create memory\n",
    "memory = ChatSummaryMemoryBuffer.from_defaults(\n",
    "    chat_history = [],\n",
    "    llm = Settings.llm,\n",
    "    token_limit = 300\n",
    ")\n",
    "history = memory.get()\n",
    "\n",
    "# update memory\n",
    "def update_memory(memory, *args) :\n",
    "    for new_chat in args :\n",
    "        memory.put(new_chat)\n",
    "    return memory, memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T02:07:06.350816Z",
     "iopub.status.busy": "2025-03-28T02:07:06.350401Z",
     "iopub.status.idle": "2025-03-28T02:07:06.361821Z",
     "shell.execute_reply": "2025-03-28T02:07:06.360544Z",
     "shell.execute_reply.started": "2025-03-28T02:07:06.350784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chatbot.py \n",
    "\n",
    "import time\n",
    "import streamlit as st\n",
    "import requests\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from qdrant_client import QdrantClient\n",
    "from mistralai import Mistral\n",
    "\n",
    "import memory\n",
    "import api_key\n",
    "import retriever\n",
    "import database\n",
    "\n",
    "# chat with Mistral\n",
    "def chat(client, query):\n",
    "    response = client.chat.stream(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    for chunk in response:\n",
    "        yield chunk.data.choices[0].delta.content or \"\"\n",
    "\n",
    "# merge query, conversation history and Qdrant response\n",
    "def merge(query, history, qdrant_response, language) :\n",
    "    history = \"\\n\".join([str(el) for el in history])\n",
    "    return f\"\"\"\n",
    "Tu es un assistant juridique expert en droit des brevets. Tu aides l'utilisateur à comprendre et à naviguer dans les aspects juridiques liés aux brevets (ex : dépôt, validité, contrefaçon, brevets européens, procédure à l'INPI ou à l'OEB, etc.), notamment dans le cadre de la préparation de concours. \n",
    "\n",
    "Tu dois conserver en mémoire tous les éléments. Pour cela, voici l'historique de la conversation ou un résumé détaillé : \n",
    "```\n",
    "{history}\n",
    "```\n",
    "\n",
    "L'utilisateur a envoyé le message suivant. C'est à celui-ci, et celui-ci uniquement, que tu vas répondre dans ta prochaine réponse. Le voici :\n",
    "```\n",
    "{query}\n",
    "```\n",
    "\n",
    "Pour y répondre, tu pourras te baser dans un premier temps sur le texte suivant, issu d'une base de connaissances spécialisée dans le domaine :\n",
    "```\n",
    "{qdrant_response}\n",
    "```\n",
    "\n",
    "Quelques autres remarques très importantes :\n",
    "- Tu dois IMPÉRATIVEMENT répondre dans la langue suivante, peu importe la langue dans laquelle l'utilisateur écrit ses messages : {language}. \n",
    "- Ta réponse devra être claire, précise, pédagogique, et uniquement en lien avec le droit des brevets. \n",
    "- Si le texte fourni (issue de la base de données) n'a rien à voir avec la demande de l'utilisateur, tu pourras dans ce cas y répondre par tes connaissances personnelles, mais tu devras impérativement le préciser de manière explicite à l'utilisateur. \n",
    "- Si tu n'as pas la réponse à la question de l'utilisateur, tu lui indiqueras clairement et honnêtement plutôt que de l'inventer. \n",
    "- Termine impérativement ton prompt par \"**Sources** :\", je mettrais les sources en-dessous (tu peux évidemment changer la langue si besoin)\n",
    "\"\"\"\n",
    "\n",
    "# initialisation\n",
    "st.title(\"CYentists' chatbot\")\n",
    "if \"session_started\" not in st.session_state :\n",
    "    Settings.embed_model = HuggingFaceEmbedding(model_name=\"baai/bge-small-en-v1.5\")\n",
    "    st.session_state.mistral_client = Mistral(api_key=api_key.MISTRAL_API_KEY)\n",
    "    st.session_state.qdrant_client = QdrantClient(\n",
    "        \"https://3cf7f5eb-3647-41f9-94aa-6dbc76a973d4.us-west-1-0.aws.cloud.qdrant.io:6333\",\n",
    "        port = 6333,\n",
    "        api_key = api_key.QDRANT_API_KEY\n",
    "    )\n",
    "    st.session_state.qdrant_db = database.Qdrant_DB(st.session_state.qdrant_client)\n",
    "    st.session_state.retriever = retriever.create_full_retriever(st.session_state.qdrant_db)\n",
    "    st.session_state.session_started = True\n",
    "    st.session_state.bot_memory = memory.memory\n",
    "    st.session_state.summarized_history = memory.history\n",
    "    st.session_state.complete_history = []\n",
    "    st.session_state.langue = \"Français\"\n",
    "    st.session_state.previous_retriever_name = \"\"\n",
    "    \n",
    "# display messages\n",
    "for msg in st.session_state.complete_history :\n",
    "    with st.chat_message(msg[\"role\"]) :\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# option buttons (select languages, select retriever and clear the conversation)\n",
    "st.sidebar.selectbox(\"Choix de la langue\", [\"Français\", \"English\", \"Deutsch\"], key = \"language\")\n",
    "st.sidebar.selectbox(\"Choix du retriever\", [\"Embedding/BM25 retrievers + rerank\", \"Embedding retriever\", \"BM25 retriever\"], key = \"retriever_name\")\n",
    "st.sidebar.slider(\"Top-K pour l'Embedding Retriever\", min_value=2, max_value=150, value=50, key=\"top_k_embedding\")\n",
    "st.sidebar.slider(\"Top-K pour le BM25 Retriever\", min_value=2, max_value=30, value=15, key=\"top_k_bm25\")\n",
    "st.sidebar.slider(\"Top-N pour le Reranker\", min_value=2, max_value=10, value=6, key=\"top_n_rerank\")\n",
    "show_scores = st.sidebar.checkbox(\"Afficher les scores de pertinence des sources\", value=False)\n",
    "\n",
    "if st.sidebar.button(\"Effacer la conversation\") :\n",
    "    st.session_state.bot_memory = memory.memory\n",
    "    st.session_state.summarized_history = memory.history\n",
    "    st.session_state.complete_history = []\n",
    "    st.rerun()\n",
    "\n",
    "# chat with the LLM\n",
    "query = st.chat_input(\"Say something...\")\n",
    "if query is not None and query.strip() != \"\" :\n",
    "\n",
    "    with st.chat_message(\"user\") :\n",
    "        st.markdown(query)\n",
    "\n",
    "    if (\n",
    "        st.session_state.previous_retriever_name != st.session_state.retriever_name or\n",
    "        st.session_state.previous_top_k_embedding != st.session_state.top_k_embedding or\n",
    "        st.session_state.previous_top_k_bm25 != st.session_state.top_k_bm25 or\n",
    "        st.session_state.previous_top_n_rerank != st.session_state.top_n_rerank\n",
    "    ):\n",
    "        st.session_state.retriever = retriever.create_chosen_retriever(\n",
    "            st.session_state.qdrant_db, \n",
    "            retriever_name=st.session_state.retriever_name, \n",
    "            emb_k=st.session_state.top_k_embedding, \n",
    "            bm25_k=st.session_state.top_k_bm25, \n",
    "            reranker_n=st.session_state.top_n_rerank\n",
    "        ) \n",
    "        \n",
    "        st.session_state.previous_retriever_name = st.session_state.retriever_name\n",
    "        st.session_state.previous_top_k_embedding = st.session_state.top_k_embedding\n",
    "        st.session_state.previous_top_k_bm25 = st.session_state.top_k_bm25\n",
    "        st.session_state.previous_top_n_rerank = st.session_state.top_n_rerank\n",
    "    \n",
    "        st.write(f\"Retriever mis à jour : {st.session_state.retriever_name} (Embedding k={st.session_state.top_k_embedding}, BM25 k={st.session_state.top_k_bm25}, Reranker n={st.session_state.top_n_rerank})\")\n",
    "\n",
    "    \n",
    "    context, sources = retriever.retrieve_context_and_sources(st.session_state.retriever, query, show_scores)\n",
    "    str_sources = \"\\n\" + \"\\n\".join(f\"- {src}\" for src in sources)\n",
    "    prompt = merge(query, st.session_state.summarized_history, context, st.session_state.language)\n",
    "    time.sleep(1.1)\n",
    "    \n",
    "    with st.chat_message(\"assistant\"):\n",
    "        full_response = \"\"\n",
    "        resp_area = st.empty()\n",
    "        for token in chat(st.session_state.mistral_client, prompt):\n",
    "            full_response += token\n",
    "            resp_area.markdown(full_response)\n",
    "        for token in str_sources :\n",
    "            full_response += token\n",
    "            resp_area.markdown(full_response)\n",
    "            time.sleep(0.02)\n",
    "        mistral_response = full_response\n",
    "    \n",
    "    st.session_state.bot_memory, st.session_state.summarized_history = memory.update_memory(\n",
    "        st.session_state.bot_memory, \n",
    "        ChatMessage(role = \"user\", content = query), \n",
    "        ChatMessage(role = \"assistant\", content = str(mistral_response))\n",
    "    )\n",
    "    \n",
    "    st.session_state.complete_history.append({\"role\" : \"user\", \"content\" : query})\n",
    "    st.session_state.complete_history.append({\"role\" : \"assistant\", \"content\" : mistral_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-28T02:41:06.375Z",
     "iopub.execute_input": "2025-03-28T02:07:10.141894Z",
     "iopub.status.busy": "2025-03-28T02:07:10.141517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.204.85.145\n",
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://nice-coins-open.loca.lt\n"
     ]
    }
   ],
   "source": [
    "!curl ipv4.icanhazip.com\n",
    "!streamlit run chatbot.py &>./logs.txt & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
